# 多模态与大文件智能处理计划

我们将实现一个智能的文件处理流程：针对小文件直接读取内容，针对大文件（>5MB）自动启用 RAG 检索，针对图片/视频启用多模态理解。

## 核心逻辑架构

### 1. 文件处理策略分发 (Backend)

在 `ChatService` 中，我们将根据**文件类型**和**文件大小**采取不同的处理策略：

*   **策略 A: 多模态直传 (Kimi)**
    *   **触发条件**：文件为图片/视频，且当前模型在支持名单中。
    *   **行为**：构造 `image_url`/`video_url` 消息，直接传给模型。

*   **策略 B: 全文上下文 (Full Context)**
    *   **触发条件**：文件为文档 (PDF/TXT/MD等)，且大小 **<= 5MB**。
    *   **行为**：直接读取文件全文，拼接到 Prompt 的上下文或用户消息中。

*   **策略 C: 自动 RAG (Auto-RAG)**
    *   **触发条件**：文件为文档，且大小 **> 5MB**。
    *   **行为**：
        1.  自动将文件转存并进行向量化处理（复用 RAG Service）。
        2.  获取生成的 `file_id`。
        3.  将该 `file_id` 动态加入到本次对话的检索范围 (`file_ids`) 中。
        4.  利用现有的 RAG 检索流程，找出与用户问题最相关的片段，拼接到 Prompt 中。

### 2. 配置与校验

*   **多模态名单**：`MULTIMODAL_CONFIG = {"kimi": ["image", "video"], ...}`。
*   **大小阈值**：`MAX_DIRECT_READ_SIZE = 5 * 1024 * 1024` (5MB)。

## 实施步骤

### Phase 1: 后端逻辑升级 (Backend)

1.  **ChatService 改造**：
    *   在 `astream_chat_with_model` 入口处遍历 `files`。
    *   **大小检查**：计算 Base64 解码后的大小。
    *   **分支处理**：
        *   **多模态**：调用 `_build_multimodal_message`。
        *   **小文档**：调用 `_read_file_content`，将文本追加到 `context_append` 变量。
        *   **大文档**：调用 `rag_service.process_uploaded_file`，获得 `file_id`，追加到 `request.file_ids`。
2.  **Prompt 调整**：
    *   确保从小文档读取的全文内容能正确注入到 `qa_prompt` 的 `{context}` 或 `{message}` 中。

### Phase 2: 前端交互升级 (Frontend)

1.  **输入框与预览**：
    *   实现粘贴/选择文件功能。
    *   显示文件大小，如果在上传前能判断 >5MB，可以给用户一个“将启用深度检索”的提示（可选）。
2.  **数据发送**：
    *   将文件转为 Base64 发送。
    *   处理后端可能的延迟（特别是大文件向量化时），确保 Loading 状态正常。

### Phase 3: 验证

1.  **图片测试**：发送图片给 Kimi，确认能描述。
2.  **小文档测试**：发送 1MB 的 PDF，确认模型能基于全文回答。
3.  **大文档测试**：发送 >5MB 的 PDF，观察后台日志确认触发了向量化流程，并基于检索片段回答。

